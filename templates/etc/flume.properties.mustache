agent.sources = kafka-source
agent.channels = file-channel
agent.sinks = hdfs-sink

# Use file storage for buffering data.
agent.channels.file-channel.type = file

# Pull data from Kafka (which rsyslog is pumping data into).
agent.sources.kafka-source.type = org.apache.flume.source.kafka.KafkaSource
agent.sources.kafka-source.channels = file-channel
agent.sources.kafka-source.zookeeperConnect = 127.0.0.1:2181
agent.sources.kafka-source.groupId = api-umbrella-flume
agent.sources.kafka-source.topic = api_umbrella_logs

# Extract the request_at timestamp from the data. Expected to be the first
# column of data.
agent.sources.tcp-listener.interceptors = timestamp-extractor
agent.sources.tcp-listener.interceptors.timestamp-extractor.type = regex_extractor
agent.sources.tcp-listener.interceptors.timestamp-extractor.regex = ^(\\d+)
agent.sources.tcp-listener.interceptors.timestamp-extractor.serializers = timestamp-serializer
agent.sources.tcp-listener.interceptors.timestamp-extractor.serializers.timestamp-serializer.type = org.apache.flume.interceptor.RegexExtractorInterceptorPassThroughSerializer
agent.sources.tcp-listener.interceptors.timestamp-extractor.serializers.timestamp-serializer.name = timestamp

# Write data to HDFS.
agent.sinks.hdfs-sink.type = hdfs
agent.sinks.hdfs-sink.channel = file-channel
agent.sinks.hdfs-sink.hdfs.path = /tmp/flume-test/take2
# Use "_" prefix for files currently being written to. This prevent Hive from
# picking these files up before they're fully written (otherwise, the partial
# gzip state results in invalid data).
agent.sinks.hdfs-sink.hdfs.inUsePrefix = _
# Write compressed tsv files.
agent.sinks.hdfs-sink.hdfs.fileType = CompressedStream
agent.sinks.hdfs-sink.hdfs.codeC = gzip
agent.sinks.hdfs-sink.hdfs.fileSuffix = .tsv.gz
# Write out the file ever 30 seconds.
agent.sinks.hdfs-sink.hdfs.rollInterval = 30
agent.sinks.hdfs-sink.hdfs.rollSize = 0
agent.sinks.hdfs-sink.hdfs.rollCount = 0
