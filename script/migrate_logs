#!/usr/bin/env ruby

# A script to migrate analytics log data from a separate ElasticSearch cluster
# to a new one (where it can be combined with other new data).
#
# This is for the migration of developer.nrel.gov's older and separate API
# Umbrella installation onto api.data.gov's newer one.
#
# Setup compressed double tunnel to directly expose ElasticSearch on DB server
# (via NAT server):
# ssh -C -L 9999:$DB_INTERNAL_IP:9200 -N $NAT_IP
#
# Run this script locally to copy data from $CURRENT_SERVER into remote
# destination DB server:
# SOURCE="http://$CURRENT_SERVER:9200" DEST="http://127.0.0.1:9999" EXPECTED_HOST="example.com" bundle exec ./script/migrate_logs

require "elasticsearch"

source_client = Elasticsearch::Client.new(:url => ENV["SOURCE"])
dest_client = Elasticsearch::Client.new(:url => ENV["DEST"])

# Manually sync the geocoding index by explicitly merging to pick the most
# recent result for a location.
source_index = "api-umbrella"
dest_index = "api-umbrella"
puts "#{Time.now}: #{source_index} => #{dest_index}"
source_geocodes = source_client.search(:index => source_index, :size => 100_000)["hits"]["hits"]
existing_geocodes = dest_client.search(:index => dest_index, :size => 100_000)["hits"]["hits"]
source_geocodes.each do |source_geocode|
  existing_geocode = existing_geocodes.detect { |dg| dg["_id"] == source_geocode["_id"] }
  if(existing_geocode && existing_geocode["_source"]["updated_at"] >= source_geocode["_source"]["updated_at"])
    next
  end

  dest_client.index(:index => dest_index, :type => source_geocode["_type"], :id => source_geocode["_id"], :body => source_geocode["_source"])
end

indices = source_client.indices.get_aliases.keys.sort
indices.each do |source_index| # rubocop:disable Lint/ShadowingOuterLocalVariable
  dest_index = source_index.dup
  if(source_index =~ /^api-umbrella-logs-v1-production-\d\d\d\d-\d\d$/)
    dest_index.gsub!(/-production-/, "-")
  elsif(source_index == "api-umbrella")
    # The geocoding collection - Migrated manually above to pick out most
    # recent from duplicates.
    next
  else
    puts "ERROR: Unknown index: #{source_index}"
    next
  end

  puts "#{Time.now}: #{source_index} => #{dest_index}"

  result = source_client.search(:index => source_index, :search_type => "scan", :scroll => "10m", :size => 500)
  scroll_id = result["_scroll_id"]
  while(scroll = source_client.scroll(:scroll_id => scroll_id, :scroll => "10m")) # rubocop:disable Lint/LiteralInCondition
    scroll_id = scroll["_scroll_id"]
    hits = scroll["hits"]["hits"]

    # Break when elasticsearch returns empty hits (we've reached the end).
    break if hits.empty?

    bulk_commands = []
    hits.each do |hit|
      # Ensure the record being imported has the expected host (just to double
      # check that the imported records can be clearly distinguished).
      if(!hit["_source"]["request_host"] || !hit["_source"]["request_host"].include?(ENV["EXPECTED_HOST"]))
        puts "ERROR: Record did not contain expected request_host field. Skipping. #{hit["_id"].inspect}"
        next
      end

      # Mark this record as imported, so we can distinguish the imported hits.
      hit["_source"]["imported"] = true

      bulk_commands << { :index => { :_index => dest_index, :_type => hit["_type"], :_id => hit["_id"] } }
      bulk_commands << hit["_source"]
    end

    dest_client.bulk(:body => bulk_commands)
  end

  if(source_index != "api-umbrella")
    # Create the version-less index alias names.
    alias_name = dest_index.gsub("-v1-", "-")
    alias_write_name = dest_index.gsub("-v1-", "-write-")
    indices = dest_client.indices.get_aliases
    if(indices[dest_index] && (!indices[dest_index]["aliases"] || !indices[dest_index]["aliases"][alias_name]))
      dest_client.indices.put_alias(:index => dest_index, :name => alias_name)
    end
    if(indices[dest_index] && (!indices[dest_index]["aliases"] || !indices[dest_index]["aliases"][alias_write_name]))
      dest_client.indices.put_alias(:index => dest_index, :name => alias_write_name)
    end
  end
end
